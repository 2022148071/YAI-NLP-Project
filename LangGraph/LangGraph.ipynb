{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3e370566",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ë¨: C:\\Users\\rkddn\\YAI-NLP\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import operator\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (rag ëª¨ë“ˆ importë¥¼ ìœ„í•´ í•„ìš”)\n",
        "project_root = Path().resolve().parent if Path().resolve().name == \"LangGraph\" else Path().resolve()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "    print(f\"âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€ë¨: {project_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "25a39b0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = os.getenv(\"HF_API_KEY\")\n",
        "if hf_token:\n",
        "    os.environ[\"HF_API_KEY\"] = hf_token\n",
        "else:\n",
        "    print(\"âš ï¸ ê²½ê³ : HF_API_KEYì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "login(token=os.getenv(\"HF_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79439f75",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "Could not get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ğŸ“„ nlp.pdf: 7í˜ì´ì§€ ë¡œë“œ\n",
            "ğŸ“„ ì´ 7ê°œ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ\n",
            "âœ‚ï¸ ì´ 30ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ\n",
            "ğŸŒ ì„ë² ë”©: API ë°©ì‹ (BAAI/bge-m3)\n",
            "âœ… ChromaDB ì €ì¥ ì™„ë£Œ: ./chroma_db (collection: my_collection, 30ê°œ ì²­í¬)\n",
            "  ğŸ“„ highmath12.txt: 1ê°œ ë¬¸ì„œ ë¡œë“œ\n",
            "ğŸ“„ ì´ 1ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
            "âœ‚ï¸ ì´ 16ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ\n",
            "ğŸŒ ì„ë² ë”©: API ë°©ì‹ (BAAI/bge-m3)\n",
            "âœ… ChromaDB ì €ì¥ ì™„ë£Œ: ./chroma_db (collection: my_collection, 16ê°œ ì²­í¬)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x22522cf6dd0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rag.ingest import ingest_documents, ingest_pdfs\n",
        "\n",
        "ingest_pdfs(\n",
        "    pdf_paths=[\"nlp.pdf\"],\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    collection_name=\"my_collection\",\n",
        ")\n",
        "ingest_documents(\n",
        "    file_paths=[\"highmath12.txt\"],\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    collection_name=\"my_collection\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6b7ae463",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LangSmith ì¶”ì  ì„¤ì • (langchain_teddynote ì œê±° â†’ í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì„¤ì •)\n",
        "load_dotenv()\n",
        "\n",
        "# LangSmith ì¶”ì ì´ í•„ìš”í•œ ê²½ìš° ì•„ë˜ í™˜ê²½ë³€ìˆ˜ë¥¼ .envì— ì„¤ì •í•˜ê±°ë‚˜ ì§ì ‘ í™œì„±í™”\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"LangSmith_YAI\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "17ecdb8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ ChromaDB ê¸°ë°˜ ì²´ì¸ ìƒì„± ì‹œì‘...\n",
            "ğŸŒ ì„ë² ë”©: API ë°©ì‹ (BAAI/bge-m3)\n",
            "ğŸ“š ChromaDB ì—°ê²° ì™„ë£Œ: 136ê°œ ë¬¸ì„œ (collection: my_collection)\n",
            "âœ… ì²´ì¸ ìƒì„± ì™„ë£Œ!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rkddn\\YAI-NLP\\rag\\chroma.py:66: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  self.vectorstore = Chroma(\n"
          ]
        }
      ],
      "source": [
        "# ===== í”„ë¡œí† íƒ€ì…(PDF) â†’ í”„ë¡œë•ì…˜(ChromaDB) ì „í™˜ =====\n",
        "# ë³€ìˆ˜ëª…ì„ retriever / chain ìœ¼ë¡œ í†µì¼í•˜ì—¬ ì´í›„ ë…¸ë“œ ì½”ë“œ ìˆ˜ì • ìµœì†Œí™”\n",
        "\n",
        "# â”€â”€ [í”„ë¡œí† íƒ€ì…] PDF ê¸°ë°˜ (ê¸°ì¡´ ì½”ë“œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# from rag.pdf import PDFRetrievalChain\n",
        "# print(\"ğŸš€ Hugging Face ê¸°ë°˜ PDF ì²´ì¸ ìƒì„± ì‹œì‘...\")\n",
        "# _rag = PDFRetrievalChain([\"nlp.pdf\"]).create_chain()\n",
        "# print(\"âœ… ì²´ì¸ ìƒì„± ì™„ë£Œ!\")\n",
        "# retriever = _rag.retriever\n",
        "# chain = _rag.chain\n",
        "\n",
        "# â”€â”€ [í”„ë¡œë•ì…˜] ChromaDB ê¸°ë°˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from rag.chroma import ChromaRetrievalChain\n",
        "\n",
        "print(\"ğŸš€ ChromaDB ê¸°ë°˜ ì²´ì¸ ìƒì„± ì‹œì‘...\")\n",
        "_rag = ChromaRetrievalChain(\n",
        "    persist_directory=\"./chroma_db\",       # ChromaDB ì €ì¥ ê²½ë¡œ\n",
        "    collection_name=\"my_collection\",        # ì»¬ë ‰ì…˜ ì´ë¦„\n",
        "    k=10,                                   # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜\n",
        ").create_chain()\n",
        "print(\"âœ… ì²´ì¸ ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "retriever = _rag.retriever\n",
        "chain = _rag.chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "556d3f9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# retriever í…ŒìŠ¤íŠ¸ (pdf_retriever â†’ retriever ë¡œ í†µì¼)\n",
        "search_result = retriever.invoke(\"ë‹·-í”„ë¡œë•íŠ¸ ì–´í…ì…˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\")\n",
        "search_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3181b34",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” íƒ€ì… ë”•ì…”ë„ˆë¦¬, ë©”ì‹œì§€ ì‹œí€€ìŠ¤ë¥¼ ê´€ë¦¬í•˜ê³  ì¶”ê°€ ë™ì‘ ì •ì˜\n",
        "class AgentState(TypedDict):\n",
        "    # add_messages reducer í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ì‹œí€€ìŠ¤ë¥¼ ê´€ë¦¬\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a052e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, List\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    #policy: Annotated[str, \"Policy\"] #ëˆ„ì ë¨\n",
        "    question: Annotated[str, \"Question\"]\n",
        "    context: Annotated[str, \"Context\"]\n",
        "    answer: Annotated[str, \"Answer\"]\n",
        "    messages: Annotated[list, add_messages] #ë©”ì‹œì§€(ëˆ„ì ë˜ëŠ” list)\n",
        "    relevance: Annotated[str, \"Relevance\"]\n",
        "\n",
        "    \"\"\"\n",
        "    context êµ¬ì„±ìš”ì†Œ\n",
        "    1. ì±—ë´‡ ëŒ€í™” ê¸°ë¡\n",
        "    2. ì›¹ ê²€ìƒ‰ ê²°ê³¼\n",
        "    3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
        "    4. [POLICY]\n",
        "    5. ì‚¬ìš©ì ì§ˆë¬¸\n",
        "    6. ìµœê·¼ì— retrieve í•œ ë‚´ìš©\n",
        "\n",
        "\n",
        "    contextë‚´ì—ì„œ ì²˜ë¦¬ or retrieve?\n",
        "    -> í”„ë¡¬í”„íŠ¸ ë‚´ì— ì—°ê´€ì„±ì´ ìˆëŠ” ë§¥ë½ì´ ìˆëŠ”ì§€ ì„œì¹˜í•˜ëŠ” LLM ëª¨ë¸ í•„ìš”í•  ë“¯\n",
        "    \"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74ace47f",
      "metadata": {},
      "source": [
        "<h3>ë…¸ë“œ ì •ì˜</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118c1b2e",
      "metadata": {},
      "source": [
        "Relevance ì²´í¬ìš© ëª¨ë¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6c9981",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d2eb90",
      "metadata": {},
      "outputs": [],
      "source": [
        "#ì¼ë‹¨ Relevanceìš© ëª¨ë¸ ì •ì˜\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.0,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "\n",
        "chat_hf = ChatHuggingFace(llm=hf_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6469b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from rag.utils import format_docs\n",
        "\n",
        "# â€» langchain_teddynote.evaluator.GroundednessChecker ì œê±°\n",
        "# â€» langchain_teddynote.messages.messages_to_history ì œê±°\n",
        "\n",
        "def retrieve_or_not(state: GraphState) -> str:\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ê²€ìƒ‰(retrieve)ì´ í•„ìš”í•œì§€ íŒë‹¨.\n",
        "    - ê²€ìƒ‰ ë¶ˆí•„ìš” â†’ \"not retrieve\" â†’ llm_answerë¡œ ì§í–‰\n",
        "    - ê²€ìƒ‰ í•„ìš” â†’ \"retrieve\" â†’ retrieve ë…¸ë“œë¡œ ì´ë™\n",
        "    Hugging Face Meta Llama(chat_hf) ì‚¬ìš©.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "    if not question:\n",
        "        return \"not retrieve\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "                ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ë ¤ë©´ **ë¬¸ì„œ/ë²¡í„°DB ê²€ìƒ‰(retrieve)**ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.\n",
        "\n",
        "                íŒë‹¨ ê¸°ì¤€:\n",
        "                - ì¸ì‚¬, ê°ì •, ë‹¨ìˆœ ëŒ€í™”(\"ì•ˆë…•\", \"ê³ ë§ˆì›Œ\", \"ë­í•´\" ë“±), ì¡ë‹´ â†’ ê²€ìƒ‰ ë¶ˆí•„ìš”\n",
        "                - ë¬¸ì„œì— ìˆì„ ë²•í•œ ì „ë¬¸ ì§€ì‹ ì§ˆë¬¸ â†’ ê²€ìƒ‰ í•„ìš”\n",
        "                - ìµœì‹  ì •ë³´/ë‰´ìŠ¤(ë…¸ë²¨ìƒ, ë‚ ì§œë³„ ì‚¬ê±´ ë“±) â†’ ê²€ìƒ‰ í•„ìš”\n",
        "\n",
        "                ì§ˆë¬¸: {question}\n",
        "\n",
        "                *ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ì¶œë ¥.\n",
        "                {{\"need_retrieve\": \"yes\"}} ë˜ëŠ” {{\"need_retrieve\": \"no\"}}\n",
        "                \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = chat_hf.invoke(prompt)\n",
        "        text = (response.content or \"\").strip()\n",
        "        match = re.search(r\"\\{[^{}]*\\\"need_retrieve\\\"[^{}]*\\}\", text)\n",
        "        if match:\n",
        "            data = json.loads(match.group(0))\n",
        "            need = (data.get(\"need_retrieve\") or \"no\").lower()\n",
        "            if need in (\"yes\", \"true\", \"1\"):\n",
        "                print(\"==============retrieve Node============\")\n",
        "                return \"retrieve\"\n",
        "        print(\"==============go to llm_answer============\")\n",
        "        return \"not retrieve\"\n",
        "    except Exception:\n",
        "        return \"retrieve\"\n",
        "\n",
        "\n",
        "def upload_to_vectorDB(state: GraphState) -> bool:\n",
        "    if \"Search í•¨ìˆ˜ì—ì„œ ì°¾ì€ ë‚´ìš©ì„ -> ë²¡í„°DBì— ì—…ë¡œë“œ ì„±ê³µ\":\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def multiturn_chat_to_vectorDB(state: GraphState) -> bool:\n",
        "    if \"ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì ¸ì„œ -> ë²¡í„°DBì— ì—…ë¡œë“œ ì„±ê³µ\":\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "\"\"\"===========ë…¸ë“œ ì •ì˜==========\"\"\"\n",
        "\n",
        "\n",
        "def retrieve(state: GraphState) -> GraphState:\n",
        "    \"\"\"ChromaDB (ë˜ëŠ” PDF) retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
        "    latest_question = state[\"question\"]\n",
        "    retrieved_docs = retriever.invoke(latest_question)  # pdf_retriever â†’ retriever\n",
        "    formatted_docs = format_docs(retrieved_docs)\n",
        "    return GraphState(context=formatted_docs)\n",
        "\n",
        "def llm_answer(state: GraphState) -> GraphState:\n",
        "    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "    latest_question = state[\"question\"]\n",
        "\n",
        "    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "    context = state[\"context\"]\n",
        "\n",
        "    # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    chat_history = state.get(\"messages\", [])\n",
        "    \n",
        "    try:\n",
        "        response = chain.invoke(  # pdf_chain â†’ chain\n",
        "            {\n",
        "                \"question\": latest_question,\n",
        "                \"context\": context,\n",
        "                \"chat_history\": chat_history,\n",
        "            }\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # BadRequestError ë“± ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ì¶œë ¥\n",
        "        print(\"=\" * 80)\n",
        "        print(\"âŒ LLM í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ!\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}\")\n",
        "        print(f\"ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}\")\n",
        "        \n",
        "        if hasattr(e, 'response'):\n",
        "            resp = e.response\n",
        "            print(f\"\\nAPI ì‘ë‹µ ê°ì²´: {resp}\")\n",
        "            try:\n",
        "                if hasattr(resp, 'text'):\n",
        "                    print(f\"API ì‘ë‹µ ë³¸ë¬¸ (text): {resp.text}\")\n",
        "                elif hasattr(resp, 'content'):\n",
        "                    print(f\"API ì‘ë‹µ ë³¸ë¬¸ (content): {resp.content}\")\n",
        "                if hasattr(resp, 'json'):\n",
        "                    try:\n",
        "                        error_json = resp.json()\n",
        "                        print(f\"API ì‘ë‹µ ë³¸ë¬¸ (JSON): {json.dumps(error_json, indent=2, ensure_ascii=False)}\")\n",
        "                    except:\n",
        "                        pass\n",
        "            except Exception as resp_e:\n",
        "                print(f\"ì‘ë‹µ ë³¸ë¬¸ ì½ê¸° ì‹¤íŒ¨: {resp_e}\")\n",
        "        \n",
        "        if hasattr(e, 'status_code'):\n",
        "            print(f\"HTTP ìƒíƒœ ì½”ë“œ: {e.status_code}\")\n",
        "        if hasattr(e, 'request_id'):\n",
        "            print(f\"Request ID: {e.request_id}\")\n",
        "        \n",
        "        context_str = str(context)\n",
        "        print(\"\\nì…ë ¥ ë°ì´í„° ì •ë³´:\")\n",
        "        print(f\"  - question ê¸¸ì´: {len(latest_question)} ë¬¸ì\")\n",
        "        print(f\"  - context ê¸¸ì´: {len(context_str)} ë¬¸ì\")\n",
        "        estimated_tokens = len(context_str) // 2\n",
        "        print(f\"  - context ì¶”ì • í† í° ìˆ˜: ì•½ {estimated_tokens:,} í† í°\")\n",
        "        print(f\"  - chat_history íƒ€ì…: {type(chat_history)}, ê¸¸ì´: {len(chat_history) if isinstance(chat_history, list) else 'N/A'}\")\n",
        "        print(f\"\\n  - context ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì): {context_str[:200]}...\")\n",
        "        print(\"=\" * 80)\n",
        "        raise\n",
        "    \n",
        "    # ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸, ë‹µë³€) ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "    return GraphState(\n",
        "        answer=response, messages=[(\"user\", latest_question), (\"assistant\", response)]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ed0f7a6",
      "metadata": {},
      "source": [
        "Relevance Check Node (question-retrieve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b13de9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def relevance_check(state: GraphState) -> GraphState:\n",
        "    prompt = f\"\"\"\n",
        "You are a grader assessing whether a retrieved document is relevant to the given question.\n",
        "Return ONLY valid JSON like: {{\"score\": \"yes\"}} or {{\"score\": \"no\"}}.\n",
        "\n",
        "Question:\n",
        "{state[\"question\"]}\n",
        "\n",
        "Retrieved document:\n",
        "{state[\"context\"]}\n",
        "\"\"\".strip()\n",
        "\n",
        "    # HF ëª¨ë¸ í˜¸ì¶œ\n",
        "    response = chat_hf.invoke(prompt)\n",
        "\n",
        "    # ChatHuggingFaceëŠ” ë³´í†µ response.contentì— í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ\n",
        "    text = response.content.strip()\n",
        "\n",
        "    # JSONë§Œ ì¶”ì¶œ (ëª¨ë¸ì´ ì•ë’¤ì— í…ìŠ¤íŠ¸ë¥¼ ì„ëŠ” ê²½ìš° ëŒ€ë¹„)\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "    if match:\n",
        "        text = match.group(0)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "        score = data.get(\"score\", \"no\").lower()\n",
        "    except Exception:\n",
        "        score = \"no\"\n",
        "\n",
        "    if score not in [\"yes\", \"no\"]:\n",
        "        score = \"no\"\n",
        "\n",
        "    print(\"==== [RELEVANCE CHECK] ====\")\n",
        "    print(score)\n",
        "\n",
        "    return {\"relevance\": score}\n",
        "\n",
        "def is_relevant(state: GraphState) -> str:\n",
        "    if state[\"relevance\"] == \"yes\":\n",
        "        return \"relevant\"\n",
        "    else:\n",
        "        return \"not relevant\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d5a58a",
      "metadata": {},
      "source": [
        "Web Search Node & Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d0bf05",
      "metadata": {},
      "outputs": [],
      "source": [
        "# â€» langchain_teddynote.tools.tavily.TavilySearch ì œê±°\n",
        "#   â†’ langchain_community.tools.tavily_search.TavilySearchResults ì‚¬ìš©\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "MAX_TOKENS_PER_DOC = 1000\n",
        "CHARS_PER_TOKEN_EST = 1.5\n",
        "MAX_CHARS_PER_DOC = int(MAX_TOKENS_PER_DOC * CHARS_PER_TOKEN_EST)  # ì•½ 1500ì\n",
        "\n",
        "\n",
        "def _summarize_if_long(content: str, llm, max_chars: int = MAX_CHARS_PER_DOC) -> str:\n",
        "    \"\"\"í…ìŠ¤íŠ¸ê°€ max_charsë¥¼ ì´ˆê³¼í•˜ë©´ LLMìœ¼ë¡œ ìš”ì•½.\"\"\"\n",
        "    if len(content) <= max_chars:\n",
        "        return content\n",
        "    prompt = (\n",
        "        f\"ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ë§Œ ë‚¨ê²¨ {max_chars}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. \"\n",
        "        f\"ìš”ì•½ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ê³ , ë¶ˆí•„ìš”í•œ ë°˜ë³µì€ ì œê±°í•˜ì„¸ìš”. ìš”ì•½ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\\n\\n\"\n",
        "        f\"---\\n{content[:8000]}\\n---\"\n",
        "    )\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        text = (response.content if hasattr(response, \"content\") else str(response)).strip()\n",
        "        return text[:max_chars]\n",
        "    except Exception:\n",
        "        return content[:max_chars] + \"...\"\n",
        "\n",
        "\n",
        "def web_search(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
        "    langchain_communityì˜ TavilySearchResults ì‚¬ìš© (langchain_teddynote ì˜ì¡´ ì œê±°)\n",
        "    \"\"\"\n",
        "    tavily_tool = TavilySearchResults(\n",
        "        max_results=5,\n",
        "        search_depth=\"basic\",\n",
        "    )\n",
        "\n",
        "    search_query = state[\"question\"]\n",
        "    search_results = tavily_tool.invoke(search_query)\n",
        "    # search_results: List[dict] â†’ ê° dictì— 'url', 'content' í‚¤ ì¡´ì¬\n",
        "\n",
        "    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ context ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\n",
        "    context_parts = []\n",
        "    for result in search_results:\n",
        "        url = result.get(\"url\", \"\")\n",
        "        content = result.get(\"content\", \"\")\n",
        "\n",
        "        # ê¸´ ê²°ê³¼ëŠ” ìš”ì•½\n",
        "        content = _summarize_if_long(content, chat_hf)\n",
        "        context_parts.append(f\"{content}\\nì¶œì²˜: {url}\")\n",
        "\n",
        "    formatted_context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "    return GraphState(context=formatted_context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f1bd8ae",
      "metadata": {},
      "source": [
        "<h3>ê·¸ë˜í”„ ìƒì„±</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ba0aee",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"llm_answer\", llm_answer)\n",
        "workflow.add_node(\"relevance_check\", relevance_check)\n",
        "workflow.add_node(\"web_search\", web_search)\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    retrieve_or_not,\n",
        "    {\n",
        "        \"retrieve\": \"retrieve\",\n",
        "        \"not retrieve\": \"llm_answer\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"retrieve\", \"relevance_check\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"relevance_check\",\n",
        "    is_relevant,\n",
        "    {\n",
        "        \"relevant\": \"llm_answer\",\n",
        "        \"not relevant\": \"web_search\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"web_search\", \"llm_answer\")\n",
        "workflow.add_edge(\"llm_answer\", END)\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "memory = MemorySaver()\n",
        "\n",
        "app = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb0c76f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# â€» langchain_teddynote.graphs.visualize_graph ì œê±° â†’ ìì²´ êµ¬í˜„ ì‚¬ìš©\n",
        "from rag.graph_utils import visualize_graph\n",
        "\n",
        "visualize_graph(app)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9341f8",
      "metadata": {},
      "source": [
        "<h3>ê·¸ë˜í”„ ì‹¤í–‰</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29e160b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# â€» langchain_teddynote.messages ì œê±° â†’ rag.graph_utils ì‚¬ìš©\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from rag.graph_utils import invoke_graph, random_uuid\n",
        "\n",
        "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
        "\n",
        "inputs = GraphState(question=\"íŒ”ë€í‹°ì–´ ì ì • ì£¼ê°€ê°€ ì–¼ë§ˆì¼ê¹Œ\")\n",
        "\n",
        "invoke_graph(app, inputs, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99821a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# â€» langchain_teddynote.messages ì œê±° â†’ rag.graph_utils ì‚¬ìš©\n",
        "from rag.graph_utils import stream_graph\n",
        "\n",
        "stream_graph(app, inputs, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841c9e9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = app.get_state(config).values\n",
        "\n",
        "print(f'Question: {outputs[\"question\"]}')\n",
        "print(\"===\" * 20)\n",
        "print(f'Answer:\\n{outputs[\"answer\"]}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
