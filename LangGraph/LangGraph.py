"""
LangGraph.py RAG íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ (LangGraph ê¸°ë°˜)
====================================================

LangGraph.ipynb ë…¸íŠ¸ë¶ì˜ ì „ì²´ ê¸°ëŠ¥ì„ import ê°€ëŠ¥í•œ Python ëª¨ë“ˆë¡œ ì •ë¦¬.
stream.py ì—ì„œ import í•˜ì—¬ Streamlit ë°ëª¨ì— í™œìš©í•©ë‹ˆë‹¤.

ì‹¤í–‰ ìˆœì„œ (ì˜ì¡´ì„± ê³ ë ¤):
  0.  í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ Â· sys.path ì„¤ì •
  1.  í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env)
  2.  ë¡œê·¸ ìˆ˜ì§‘ê¸° (stream.py â†’ toast ì—°ë™)
  3.  ìƒìˆ˜ ì •ì˜
  4.  ì™¸ë¶€ íŒ¨í‚¤ì§€ import (LangChain, LangGraph ë“±)
  5.  ë¡œì»¬ ëª¨ë“ˆ import (rag íŒ¨í‚¤ì§€)
  6.  GraphState ì •ì˜
  7.  ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ (ì´ˆê¸°í™” ì‹œ ì„¤ì •)
  8.  ì´ˆê¸°í™” í•¨ìˆ˜
  9.  ë¬¸ì„œ ì ì¬ API
  10. í—¬í¼ í•¨ìˆ˜
  11. ë…¸ë“œ í•¨ìˆ˜
  12. ë¼ìš°íŒ… í•¨ìˆ˜ (conditional_edges ìš©)
  13. ê·¸ë˜í”„ êµ¬ì„± Â· ì»´íŒŒì¼
  14. ê³µê°œ API (query ë“±)
"""

# ============================================================
# 0. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ Â· ê²½ë¡œ ì„¤ì •
# ============================================================
import os
import sys
import time
import json
import re
import uuid
import numpy as np
import requests
from pathlib import Path
from datetime import datetime, timezone
from typing import TypedDict, Annotated, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

# ì´ íŒŒì¼ì€ <project_root>/LangGraph/ ì— ìœ„ì¹˜.
# rag íŒ¨í‚¤ì§€ë¥¼ import í•˜ë ¤ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ê°€ sys.path ì— ìˆì–´ì•¼ í•œë‹¤.
_PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(_PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT))

# ============================================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ============================================================
from dotenv import load_dotenv

load_dotenv()  # .env íŒŒì¼ì—ì„œ HF_API_KEY, TAVILY_API_KEY ë“± ë¡œë“œ

# ============================================================
# 2. ë¡œê·¸ ìˆ˜ì§‘ê¸°
#    - ë…¸ë“œ ë‚´ë¶€ print ëŒ€ì‹  _log() ì‚¬ìš©
#    - stream.py ì—ì„œ get_and_clear_logs() â†’ st.toast()
# ============================================================
_log_buffer: List[str] = []


def _log(msg: str):
    """ë‚´ë¶€ ë©”ì‹œì§€ë¥¼ ë²„í¼ì— ì €ì¥í•˜ê³  ì½˜ì†”ì—ë„ ì¶œë ¥"""
    _log_buffer.append(msg)
    print(msg)


def get_and_clear_logs() -> List[str]:
    """ìŒ“ì¸ ë¡œê·¸ë¥¼ ë°˜í™˜í•˜ê³  ë²„í¼ë¥¼ ë¹„ìš´ë‹¤ (stream.py ê°€ í˜¸ì¶œ)."""
    msgs = _log_buffer.copy()
    _log_buffer.clear()
    return msgs


# ============================================================
# 3. ìƒìˆ˜
# ============================================================
PERSIST_DIR = "./chroma_db"  # ChromaDB ì €ì¥ ê²½ë¡œ (LangGraph/ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
COLLECTION_MAIN = "my_collection"  # ì£¼ìš” ë¬¸ì„œ ì»¬ë ‰ì…˜
COLLECTION_CHAT_RAW = "chat_history_raw"  # ëŒ€í™” ì›ë³¸ ì €ì¥
COLLECTION_CHAT_SUMMARY = "chat_history_summarized"  # ëŒ€í™” ìš”ì•½ ì €ì¥

# LLM ëª¨ë¸ ì‹ë³„ì
# ROUTER_MODEL = "meta-llama/Llama-3.1-8B-Instruct"  # ë¼ìš°íŒ…Â·íŒë‹¨Â·ìš”ì•½ìš©
# CHAIN_MODEL = "meta-llama/Meta-Llama-3-8B-Instruct"  # ë‹µë³€ ìƒì„±ìš© (rag.base)

ROUTER_MODEL = "Qwen/Qwen2.5-14B-Instruct"  # ë¼ìš°íŒ…Â·íŒë‹¨Â·ìš”ì•½ìš©
CHAIN_MODEL = "Qwen/Qwen2.5-14B-Instruct"  # ë‹µë³€ ìƒì„±ìš© (rag.base)
EMBEDDING_MODEL = "BAAI/bge-m3"  # ì„ë² ë”© ëª¨ë¸

STYLE_MODELS = {
    "direct": "RiverWon/NeuLoRA-direct",
    "socratic": "RiverWon/NeuLoRA-socratic",
    "scaffolding": "RiverWon/NeuLoRA-scaffolding",
    "feedback": "RiverWon/NeuLoRA-feedback",
}

MAX_CHARS_PER_DOC = 1500  # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì„ê³„ì¹˜ (â‰ˆ1000 í† í°)

# llm_answer í”„ë¡¬í”„íŠ¸ ë‚´ chat_history ê¸¸ì´ ì œí•œ
MAX_HISTORY_TURNS = 6  # ìµœê·¼ Ní„´(2Nê°œ ë©”ì‹œì§€)ê¹Œì§€ í¬í•¨
MAX_HISTORY_CHARS = 2000  # íˆìŠ¤í† ë¦¬ ë¸”ë¡ ìµœëŒ€ ë¬¸ì ìˆ˜, ì´ˆê³¼ ì‹œ ì•(ì˜¤ë˜ëœ í„´)ë¶€í„° ìƒëµ

LORA_ROUTER_PATH = Path(__file__).parent / "router_model.json"

# PEFT ì–´ëŒ‘í„°ê°€ ì„œë¸Œí´ë”ì— ìˆëŠ” ë¦¬í¬ (adapter_config.json ìˆìŒ)
# direct:   https://huggingface.co/marimmo/multi-lora/tree/main/direct
# feedback: https://huggingface.co/marimmo/multi-lora/tree/main/feedback
# scaffolding: https://huggingface.co/marimmo/multi-lora/tree/main/scaffolding
# socratic: https://huggingface.co/marimmo/multi-lora/tree/main/socratic
PEFT_REPO = "marimmo/multi-lora" 
# ============================================================
# 4. ì™¸ë¶€ íŒ¨í‚¤ì§€ import
# ============================================================
from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from langchain_chroma import Chroma
from langchain_tavily import TavilySearch
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages, REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import RemoveMessage

# ============================================================
# 5. ë¡œì»¬ ëª¨ë“ˆ import (rag íŒ¨í‚¤ì§€)

# ============================================================
from rag.base import create_embedding_auto
from rag.chroma import ChromaRetrievalChain
from rag.ingest import ingest_documents as _raw_ingest_docs
from rag.ingest import ingest_pdfs as _raw_ingest_pdfs
from rag.utils import format_docs
from rag.graph_utils import random_uuid  # ì„¸ì…˜ ID ìƒì„±ìš© (re-export)

# ============================================================
# 6. GraphState ì •ì˜
# ============================================================


class GraphState(TypedDict):
    """LangGraph ë…¸ë“œ ê°„ ì „ë‹¬ë˜ëŠ” ìƒíƒœ ë”•ì…”ë„ˆë¦¬"""

    question: Annotated[str, "ì‚¬ìš©ì ì§ˆë¬¸ (ì¬ì‘ì„± í›„ ê°±ì‹ ë¨)"]
    context: Annotated[str, "ê²€ìƒ‰Â·ì›¹ ê²°ê³¼ë¥¼ í•©ì¹œ ë¬¸ë§¥ í…ìŠ¤íŠ¸"]
    answer: Annotated[str, "LLM ì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€"]
    messages: Annotated[list, add_messages]  # ëŒ€í™” ì´ë ¥ (ëˆ„ì )
    relevance: Annotated[str, "ê²€ìƒ‰ ë¬¸ì„œ ê´€ë ¨ì„± yes/no"]
    policy: Annotated[str, "í•™ìƒì— ëŒ€í•œ ë‹µì•ˆ ë°©í–¥ì„±"]


# ============================================================
# 7. ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ â€” initialize() ì—ì„œ ì„¤ì •ë¨
# ============================================================
_peft_model = None   # PEFT ì–´ëŒ‘í„° ì‚¬ìš© ì‹œì—ë§Œ ì„¤ì •; ì „ì²´ ëª¨ë¸ ë¡œë“œ ì‹œ None
_rag_llm = None
_tokenizer = None
_use_peft_adapters = False  # True: set_adapter() ì‚¬ìš© / False: ë‹¨ì¼ ì „ì²´ ëª¨ë¸
_retriever = None  # ChromaDB ê¸°ë°˜ retriever
_chain = None  # RAG ë‹µë³€ ì²´ì¸
_chat_hf = None  # ë¼ìš°íŒ…Â·íŒë‹¨Â·ìš”ì•½ìš© LLM
_embeddings = None  # ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
_app = None  # ì»´íŒŒì¼ëœ LangGraph ì•±
_initialized = False  # ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸
_answer_model_used: str | None = None  # ì‹¤ì œ ì²´ì¸ ìƒì„±ì— ì‚¬ìš©ëœ ë‹µë³€ ëª¨ë¸ëª…
_bg_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="bg-db")

# ============================================================
# 8. ì´ˆê¸°í™” í•¨ìˆ˜
# ============================================================


def _init_hf_login():
    """HuggingFace Hub í† í° ë¡œê·¸ì¸"""
    from huggingface_hub import login

    token = os.getenv("HF_API_KEY")
    if token:
        os.environ["HF_API_KEY"] = token
        login(token=token)
        _log("âœ… HuggingFace ë¡œê·¸ì¸ ì„±ê³µ")
    else:
        _log("âš ï¸ HF_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")


def _make_vessel_chat_model():
    """
    vessel(ë¡œì»¬ GPU)ìš© LLM ìƒì„±.
    transformers íŒŒì´í”„ë¼ì¸ â†’ LangChain í˜¸í™˜ ë˜í¼(.invoke() ë°˜í™˜ê°’ì— .content ìˆìŒ).
    """
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
    from langchain_huggingface import HuggingFacePipeline

    model_kwargs = {"device_map": "auto", "dtype": "auto"}
    q4 = os.getenv("LLM_4BIT", "").lower() in ("1", "true", "yes")
    q8 = os.getenv("LLM_8BIT", "").lower() in ("1", "true", "yes")
    if q4:
        model_kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype="float16",
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        _log("âš–ï¸ ë‹µë³€ LLM 4bit ì–‘ìí™” ì‚¬ìš©")
    elif q8:
        model_kwargs["quantization_config"] = BitsAndBytesConfig(load_in_8bit=True)
        _log("âš–ï¸ ë‹µë³€ LLM 8bit ì–‘ìí™” ì‚¬ìš©")

    model = AutoModelForCausalLM.from_pretrained(ROUTER_MODEL, **model_kwargs)
    tokenizer = AutoTokenizer.from_pretrained(ROUTER_MODEL)

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        temperature=0.7,
        do_sample=True,
    )
    llm = HuggingFacePipeline(pipeline=pipe)

    # ChatHuggingFace ê°€ ë¬¸ìì—´ ì¶œë ¥(llm.invoke ê²°ê³¼)ì„ AIMessage(content=...)ë¡œ ê°ì‹¸ì£¼ë¯€ë¡œ,
    # LangGraph ì „ì²´ì—ì„œ ê¸°ëŒ€í•˜ëŠ” `.invoke(...).content` ì¸í„°í˜ì´ìŠ¤ì™€ ê·¸ëŒ€ë¡œ í˜¸í™˜ëœë‹¤.
    return ChatHuggingFace(llm=llm)


def _init_chat_model():
    """ë¼ìš°íŒ…Â·íŒë‹¨Â·ìš”ì•½ìš© LLM ì´ˆê¸°í™”. LLM_MODE=vessel ì´ë©´ ë¡œì»¬ GPU, ì•„ë‹ˆë©´ API."""
    global _chat_hf

    mode = (os.getenv("LLM_MODE") or "api").strip().lower()

    if mode == "vessel":
        _chat_hf = _make_vessel_chat_model()
        _log(f"âœ… ë¼ìš°íŒ… LLM ë¡œë“œ ì™„ë£Œ (vessel ë¡œì»¬): {ROUTER_MODEL}")
    else:
        llm = HuggingFaceEndpoint(
            repo_id=ROUTER_MODEL,
            task="text-generation",
            temperature=0.7,
            max_new_tokens=1024,
        )
        _chat_hf = ChatHuggingFace(llm=llm)
        _log(f"âœ… ë¼ìš°íŒ… LLM ë¡œë“œ ì™„ë£Œ (API): {ROUTER_MODEL}")


def _init_embeddings():
    """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (create_embedding_auto â†’ ë¡œì»¬/API ìë™ ì„ íƒ)"""
    global _embeddings
    _embeddings = create_embedding_auto()
    _log(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {EMBEDDING_MODEL}")


def _init_rag_chain(
    persist_directory: str = PERSIST_DIR,
    collection_name: str = COLLECTION_MAIN,
    k: int = 10,
):
    """ChromaDB ê¸°ë°˜ RAG ì²´ì¸ (retriever + chain) ì´ˆê¸°í™”"""
    global _retriever, _chain, _answer_model_used
    _log("ğŸš€ ChromaDB ê¸°ë°˜ RAG ì²´ì¸ ìƒì„± ì‹œì‘...")
    rag = ChromaRetrievalChain(
        persist_directory=persist_directory,
        collection_name=collection_name,
        k=k,
    ).create_chain()
    _retriever = rag.retriever
    _chain = rag.chain
    _log("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")

def _init_peft_model():
    """8bit/4bit Multi-LoRA ë˜ëŠ” RiverWon/NeuLoRA-direct ê°™ì€ ì „ì²´ ëª¨ë¸ ë¡œë“œ"""
    global _peft_model, _rag_llm, _tokenizer, _use_peft_adapters
    from transformers import (
        AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
    )
    from langchain_community.llms import HuggingFacePipeline
    from peft import PeftModel, TaskType
    import torch
    import requests  # marimmo router ë‹¤ìš´ë¡œë“œ
    
    quant = os.getenv("LLM_QUANT", "8bit").lower()
    # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPU ì˜¤í”„ë¡œë“œ í—ˆìš© (VRAM ì‘ì„ ë•Œ)
    enable_cpu_offload = os.getenv("LLM_CPU_OFFLOAD", "").lower() in ("1", "true", "yes")

    # ì–‘ìí™” config
    if quant == "4bit":
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        _log("ğŸ”§ 4bit ì–‘ìí™” í™œì„± (OOM ëŒ€ë¹„)")
    else:  # 8bit ìš°ì„ 
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weights=False,
            bnb_8bit_compute_dtype=torch.bfloat16,
            bnb_8bit_use_double_quant=True,
            llm_int8_enable_fp32_cpu_offload=enable_cpu_offload,
        )
        if enable_cpu_offload:
            _log("ğŸ”§ 8bit + CPU ì˜¤í”„ë¡œë“œ í™œì„± (VRAM ë¶€ì¡± ì‹œ ì¼ë¶€ ë ˆì´ì–´ CPU)")
    
    # ë² ì´ìŠ¤ 14B ë¡œë“œ (flash_attn ë¯¸ì‚¬ìš© ì‹œ sdpa/eager ì‚¬ìš© â€” PyTorchÂ·flash_attn ABI ë¶ˆì¼ì¹˜ íšŒí”¼)
    attn_impl = (os.getenv("ATTN_IMPLEMENTATION") or "sdpa").strip().lower()
    if attn_impl == "flash_attention_2":
        try:
            import flash_attn  # noqa: F401
        except (ImportError, OSError) as e:
            _log(f"âš ï¸ flash_attention_2 ë¡œë“œ ì‹¤íŒ¨ ({e}) â†’ sdpa ì‚¬ìš©")
            attn_impl = "sdpa"
    if attn_impl not in ("flash_attention_2", "sdpa", "eager"):
        attn_impl = "sdpa"
    _log(f"ğŸ”§ attention êµ¬í˜„: {attn_impl}")

    # CPU ì˜¤í”„ë¡œë“œ ì‹œ device_map + max_memoryë¡œ GPU í•œë„ ì§€ì • í›„ ë‚˜ë¨¸ì§€ëŠ” CPU
    if enable_cpu_offload and quant != "4bit":
        max_memory = {0: os.getenv("LLM_GPU_MAX_MEMORY", "20GiB"), "cpu": "30GiB"}
        device_map = "auto"
        _log(f"ğŸ”§ device_map=auto, max_memory={max_memory}")
    else:
        max_memory = None
        device_map = "auto"

    model = AutoModelForCausalLM.from_pretrained(
        CHAIN_MODEL,
        quantization_config=bnb_config,
        device_map=device_map,
        max_memory=max_memory,
        torch_dtype=torch.bfloat16,
        attn_implementation=attn_impl,
        low_cpu_mem_usage=True,
    )
    _tokenizer = AutoTokenizer.from_pretrained(CHAIN_MODEL)
    _tokenizer.pad_token = _tokenizer.eos_token

    # 1) marimmo/multi-lora ì„œë¸Œí´ë”(direct, socratic ë“±)ì—ì„œ PEFT ì–´ëŒ‘í„° ë¡œë“œ ì‹œë„ (adapter_config.json ìˆìŒ)
    # 2) ì‹¤íŒ¨ ì‹œ RiverWon/NeuLoRA-* ê°œë³„ ë¦¬í¬ë¥¼ PEFTë¡œ ì‹œë„
    # 3) ê·¸ë„ ì‹¤íŒ¨ ì‹œ RiverWon/NeuLoRA-direct ë¥¼ ì „ì²´ ëª¨ë¸ë¡œ ë¡œë“œ
    run_model = None
    try:
        _peft_model = PeftModel.from_pretrained(
            model,
            PEFT_REPO,
            subfolder="direct",
            task_type=TaskType.CAUSAL_LM,
            adapter_name="direct",
        )
        for style in ["socratic", "scaffolding", "feedback"]:
            _peft_model.load_adapter(PEFT_REPO, adapter_name=style, subfolder=style)
        _use_peft_adapters = True
        run_model = _peft_model
        _log(f"âœ… 14B {quant} Multi-LoRA ë¡œë“œ (ì–´ëŒ‘í„°: marimmo/multi-lora direct/socratic/scaffolding/feedback)")
    except Exception as e1:
        _log(f"â„¹ï¸ marimmo/multi-lora ì„œë¸Œí´ë” ë¡œë“œ ì‹¤íŒ¨: {e1}")
        try:
            _peft_model = PeftModel.from_pretrained(
                model,
                STYLE_MODELS["direct"],
                task_type=TaskType.CAUSAL_LM,
                adapter_name="direct",
            )
            for style, path in list(STYLE_MODELS.items())[1:]:
                _peft_model.add_adapter(path, adapter_name=style)
            _use_peft_adapters = True
            run_model = _peft_model
            _log(f"âœ… 14B {quant} Multi-LoRA ë¡œë“œ (ì–´ëŒ‘í„°: {list(STYLE_MODELS)})")
        except Exception as e2:
            err_msg = str(e2)
            if "adapter_config" not in err_msg and "Entry Not Found" not in err_msg and "EntryNotFoundError" not in type(e2).__name__:
                raise
            _log(f"âš ï¸ PEFT ì–´ëŒ‘í„° ì—†ìŒ â†’ {STYLE_MODELS['direct']} ì „ì²´ ëª¨ë¸ë¡œ ë¡œë“œ")
            del model
            torch.cuda.empty_cache()
            model = AutoModelForCausalLM.from_pretrained(
                STYLE_MODELS["direct"],
                quantization_config=bnb_config,
                device_map=device_map,
                max_memory=max_memory,
                torch_dtype=torch.bfloat16,
                attn_implementation=attn_impl,
                low_cpu_mem_usage=True,
            )
            _tokenizer = AutoTokenizer.from_pretrained(STYLE_MODELS["direct"])
            _tokenizer.pad_token = _tokenizer.eos_token
            _peft_model = None
            _use_peft_adapters = False
            run_model = model
            _log(f"âœ… 14B {quant} ì „ì²´ ëª¨ë¸ ë¡œë“œ: {STYLE_MODELS['direct']}")

    if run_model is None:
        raise RuntimeError("PEFT/ì „ì²´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

    pipe = pipeline(
        "text-generation",
        model=run_model,
        tokenizer=_tokenizer,
        max_new_tokens=384,
        temperature=0.7,
        do_sample=True,
        pad_token_id=_tokenizer.eos_token_id,
    )
    _rag_llm = HuggingFacePipeline(pipeline=pipe)
    torch.cuda.empty_cache()

def route_style(question: str) -> str:
    """ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•˜ê³  ê°€ì¥ ê°€ê¹Œìš´ ìŠ¤íƒ€ì¼ centroid ì„ íƒ"""
    if not LORA_ROUTER_PATH.exists():
        _log("âš ï¸ router_model.json ì—†ìŒ â†’ direct ê¸°ë³¸")
        return "direct"
    
    try:
        with open(LORA_ROUTER_PATH, "r") as f:
            data = json.load(f)
        centroids = {
            style: np.array(vec, dtype=np.float32)
            for style, vec in data.get("centroids", {}).items()
        }
    except Exception as e:
        _log(f"âš ï¸ centroids ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "direct"
    
    if not centroids or _embeddings is None:
        return "direct"
    
    query_emb = np.array(_embeddings.embed_query(question), dtype=np.float32)
    
    best_style, best_sim = "direct", -1.0
    for style, centroid in centroids.items():
        sim = np.dot(query_emb, centroid) / (
            np.linalg.norm(query_emb) * np.linalg.norm(centroid) + 1e-9
        )
        if sim > best_sim:
            best_sim = sim
            best_style = style
    
    _log(f"ğŸ“Š centroids ìœ ì‚¬ë„: {best_style}={best_sim:.3f}")
    return best_style

def initialize(
    persist_directory: str = PERSIST_DIR,
    collection_name: str = COLLECTION_MAIN,
    k: int = 10,
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” â€” ìµœì´ˆ 1 íšŒë§Œ ì‹¤í–‰.

    ìˆœì„œ: HF ë¡œê·¸ì¸ â†’ ì„ë² ë”© â†’ ë¼ìš°íŒ… LLM â†’ PEFT(RAG ë‹µë³€) LLM â†’ RAG ì²´ì¸.

    OOM ì°¸ê³ : LLM_MODE=vessel ì´ë©´ ë¼ìš°íŒ…ìš© ROUTER_MODEL(14B)ê³¼ ë‹µë³€ìš© CHAIN_MODEL(14B)ì„
    ë‘˜ ë‹¤ GPUì— ì˜¬ë¦¬ë¯€ë¡œ 24GB í•œ ì¥ìœ¼ë¡œëŠ” ë¶€ì¡±í•  ìˆ˜ ìˆìŒ. ë‹¨ì¼ GPU ì‹œ ë¼ìš°íŒ…ë§Œ APIë¡œ ë‘ê³ 
    LLM_MODE=api ê¶Œì¥ (ë‹µë³€ìš© PEFT 14Bë§Œ ë¡œì»¬).
    """
    global _initialized
    if _initialized:
        return

    _init_hf_login()
    _init_embeddings()
    # ë¼ìš°íŒ…/íŒë‹¨/ìš”ì•½ìš©: vesselì´ë©´ ë™ì¼ GPUì— 14B ì¶”ê°€ ë¡œë“œ â†’ ë‹¨ì¼ 24GBì—ì„œ PEFT 14Bì™€ í•¨ê»˜ OOM ê°€ëŠ¥
    if (os.getenv("LLM_MODE") or "").strip().lower() == "vessel":
        _log("â„¹ï¸ LLM_MODE=vessel: ë¼ìš°íŒ…ìš© 14Bë„ GPU ë¡œë“œ. ë‹¨ì¼ 24GB GPUë©´ OOM ì‹œ LLM_MODE=api ë¡œ ë¼ìš°íŒ…ë§Œ API ì‚¬ìš© ê¶Œì¥.")
    _init_chat_model()
    _init_peft_model()
    _init_rag_chain(persist_directory, collection_name, k)
    _initialized = True
    _log("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")


# ============================================================
# 9. ë¬¸ì„œ ì ì¬ API
# ============================================================


def ingest_uploaded_file(
    file_path: str,
    persist_directory: str = PERSIST_DIR,
    collection_name: str = COLLECTION_MAIN,
):
    """
    ì—…ë¡œë“œëœ ë‹¨ì¼ íŒŒì¼ (PDF / TXT) ì„ ChromaDB ì— ì ì¬.
    stream.py íŒŒì¼ ì—…ë¡œë“œì—ì„œ í˜¸ì¶œ.
    """
    ext = Path(file_path).suffix.lower()
    if ext == ".pdf":
        _raw_ingest_pdfs(
            pdf_paths=[file_path],
            persist_directory=persist_directory,
            collection_name=collection_name,
        )
    else:
        _raw_ingest_docs(
            file_paths=[file_path],
            persist_directory=persist_directory,
            collection_name=collection_name,
        )
    _log(f"âœ… íŒŒì¼ ì ì¬ ì™„ë£Œ: {Path(file_path).name}")


# ============================================================
# 10. í—¬í¼ í•¨ìˆ˜
# ============================================================


def _to_text(msg) -> str:
    """
    ë‹¤ì–‘í•œ ë©”ì‹œì§€ íƒ€ì…ì„ 'role: content' ë¬¸ìì—´ë¡œ ë³€í™˜.
    íŠ¹ìˆ˜ í† í°ì´ ì„ì—¬ìˆìœ¼ë©´ ì œê±°í•˜ì—¬ ìˆœìˆ˜ ëŒ€í™” ë‚´ìš©ë§Œ ë‚¨ê¸´ë‹¤.
    """
    if hasattr(msg, "type") and hasattr(msg, "content"):
        return f"{msg.type}: {_strip_chat_tokens(str(msg.content))}"
    if isinstance(msg, (tuple, list)) and len(msg) >= 2:
        return f"{msg[0]}: {_strip_chat_tokens(str(msg[1]))}"
    return _strip_chat_tokens(str(msg))


def _extract_question(raw) -> str:
    """state['question'] ì´ ì–´ë–¤ íƒ€ì…ì´ë“  ìˆœìˆ˜ ë¬¸ìì—´ë¡œ ì¶”ì¶œ"""
    if hasattr(raw, "content"):
        return str(raw.content)
    if isinstance(raw, (list, tuple)) and raw:
        last = raw[-1]
        return str(last.content) if hasattr(last, "content") else str(last)
    return str(raw)


def _looks_ambiguous(q: str) -> bool:
    """ì§§ê±°ë‚˜ ëŒ€ëª…ì‚¬ Â· ëª¨í˜¸ í‘œí˜„ì´ í¬í•¨ëœ ì§ˆë¬¸ì¸ì§€ íœ´ë¦¬ìŠ¤í‹± íŒë³„"""
    q = (q or "").strip()
    if not q:
        return False
    ambiguous = [
        "ê·¸ê±°", "ê·¸ê²ƒ", "ê·¸ê²Œ", "ì´ê²Œ", "ì´ê±°", "ì €ê±°", "ê·¸ë•Œ", "ì €ë²ˆ", "ì•„ê¹Œ",
        "ê·¸ ë‚´ìš©", "ê·¸ ì´ì•¼ê¸°", "ê¸°ì–µë‚˜", "ê¸°ì–µí•´", "ë‹¤ì‹œ", "ì´ì–´",
        "ë” ìì„¸íˆ", "ë­ì˜€ì§€",
    ]
    short_followups = ["ì™œ?", "ì–´ì§¸ì„œ?", "ë­ì•¼?", "ë­”ë°?", "ê·¸ê²Œ ë­ì•¼?", "ì„¤ëª…í•´ì¤˜"]
    return any(t in q for t in ambiguous) or q in short_followups or len(q) <= 8


def _message_to_role_content(msg):
    """ë©”ì‹œì§€ â†’ (role, content) íŠœí”Œ ë³€í™˜"""
    if hasattr(msg, "type") and hasattr(msg, "content"):
        role = {"human": "user", "ai": "assistant"}.get(msg.type, msg.type)
        return role, str(msg.content)
    if isinstance(msg, (tuple, list)) and len(msg) >= 2:
        return str(msg[0]), str(msg[1])
    return "unknown", str(msg)


def _conversation_only(messages) -> list:
    """user/assistant ì—­í• ì˜ ë©”ì‹œì§€ë§Œ í•„í„°ë§"""
    conv = []
    for m in messages:
        role, content = _message_to_role_content(m)
        if role in {"user", "assistant", "human", "ai"}:
            conv.append((role, content))
    return conv

def _strip_chat_tokens(text: str) -> str:
    """ChatHuggingFace ë¡œì»¬ ëª¨ë¸ì´ ì¶œë ¥ì— í¬í•¨ì‹œí‚¤ëŠ” íŠ¹ìˆ˜ í† í°Â·ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê±°"""
    import re as _re
    last_assistant = text.rfind("<|im_start|>assistant")
    if last_assistant != -1:
        text = text[last_assistant + len("<|im_start|>assistant"):]
    text = _re.sub(r"<\|im_start\|>\s*(system|user|assistant)", "", text)
    text = text.replace("<|im_end|>", "").replace("<|im_start|>", "")
    text = text.replace("<|endoftext|>", "")
    return text.strip()


def _clean_answer_for_display(raw: str) -> str:
    """
    llm_answer ìµœì¢… ë‹µë³€ì—ì„œ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ë©´ ì•ˆ ë˜ëŠ” ë‚´ìš© ì œê±°.
    - ì—­í•  ë ˆì´ë¸”(system, user, assistant, Context:, Policy:, History:)
    - ì›¹ ê²€ìƒ‰ ì¶œì²˜(ì¶œì²˜: http...)
    - í”„ë¡¬í”„íŠ¸ê°€ ê·¸ëŒ€ë¡œ ì—ì½”ëœ ë¸”ë¡
    """
    if not (raw or "").strip():
        return ""
    text = _strip_chat_tokens(raw)
    text = re.sub(r"<\|im_[a-z_]+\|>", "", text).strip()
    # ì¶œì²˜ URL ì œê±° (ì¤„ ë‹¨ìœ„ ë˜ëŠ” ë¬¸ì¥ ì¤‘ê°„)
    text = re.sub(r"\n\s*ì¶œì²˜:\s*https?://[^\n]+", "\n", text, flags=re.IGNORECASE)
    text = re.sub(r"^\s*ì¶œì²˜:\s*https?://[^\n]+", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\s*ì¶œì²˜:\s*https?://\S+", "", text, flags=re.IGNORECASE)
    # ë¼ì¸ ë‹¨ìœ„ë¡œ ì—­í• /ë©”íƒ€ ë¼ë²¨ ì œê±° (ì¤„ ì‹œì‘ì´ system, Context:, Policy:, History:, user, assistant ì¸ ê²½ìš°)
    lines = []
    for line in text.split("\n"):
        s = line.strip()
        if not s:
            lines.append("")
            continue
        if re.match(r"^(system|Context:\s*$|Policy:\s*$|History:\s*$)", s, re.IGNORECASE):
            continue
        if re.match(r"^(user|assistant)\s*$", s, re.IGNORECASE):
            continue
        if s.lower().startswith("context:") and len(s) < 80:
            continue
        if s.lower().startswith("policy:") and len(s) < 80:
            continue
        if s.lower().startswith("history:") and len(s) < 80:
            continue
        if s.lower().startswith("user "):
            s = s[5:].strip()
        if s.lower().startswith("assistant "):
            s = s[9:].strip()
        lines.append(line)
    text = "\n".join(lines)
    # ë§ˆì§€ë§‰ 'assistant ' ë¸”ë¡ë§Œ ë‚¨ê¸°ê¸° (ëª¨ë¸ì´ ì „ì²´ ëŒ€í™”ë¥¼ ì—ì½”í•œ ê²½ìš°)
    if "\nassistant " in text or "\nuser " in text:
        parts = re.split(r"\n(?:user|assistant)\s+", text, flags=re.IGNORECASE)
        if len(parts) > 1:
            text = parts[-1].strip()
    # ì•ë’¤ ê³µë°±Â·ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text


def _invoke_clean(prompt) -> str:
    """_chat_hf.invoke() í˜¸ì¶œ í›„ íŠ¹ìˆ˜ í† í°/ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê±°í•œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë°˜í™˜"""
    resp = _chat_hf.invoke(prompt)
    raw = resp.content if hasattr(resp, "content") else str(resp)
    return _strip_chat_tokens(raw)

def _summarize_if_long(content: str, max_chars: int = MAX_CHARS_PER_DOC) -> str:
    """í…ìŠ¤íŠ¸ê°€ max_chars ë¥¼ ì´ˆê³¼í•˜ë©´ _chat_hf ë¡œ ìš”ì•½"""
    if len(content) <= max_chars:
        return content
    prompt = (
        f"ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ë§Œ ë‚¨ê²¨ {max_chars}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. "
        f"í•œê¸€ë¡œ ì‘ì„±í•˜ê³  ë¶ˆí•„ìš”í•œ ë°˜ë³µì€ ì œê±°í•˜ì„¸ìš”. ìš”ì•½ë§Œ ì¶œë ¥.\n\n"
        f"---\n{content[:8000]}\n---"
    )
    try:
        text = _invoke_clean(prompt)
        return text[:max_chars]
    except Exception:
        return content[:max_chars] + "..."


# ============================================================
# 11. ë…¸ë“œ í•¨ìˆ˜
# ============================================================


def _timed_node(node_func, node_name: str):
    """
    ë…¸ë“œ ì§„ì… ì‹œÂ·í‡´ì¥ ì‹œ ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” ë˜í¼.
    ì§„ì… ì‹œ t0 ê¸°ë¡ â†’ ì›ë³¸ ë…¸ë“œ ì‹¤í–‰ â†’ í‡´ì¥ ì‹œ ì†Œìš” ì‹œê°„ ë¡œê·¸.
    """
    def wrapped(state: GraphState) -> GraphState:
        t0 = time.perf_counter()
        _log(f"â±ï¸ [{node_name}] ì§„ì… @ {t0:.3f}s")
        try:
            out = node_func(state)
            t1 = time.perf_counter()
            elapsed = t1 - t0
            _log(f"â±ï¸ [{node_name}] í‡´ì¥ @ {t1:.3f}s (ì†Œìš”: {elapsed:.3f}s)")
            return out
        except Exception as e:
            t1 = time.perf_counter()
            _log(f"â±ï¸ [{node_name}] ì˜ˆì™¸ë¡œ í‡´ì¥ @ {t1:.3f}s (ì†Œìš”: {(t1 - t0):.3f}s) â€” {e}")
            raise
    return wrapped


def contextualize(state: GraphState) -> GraphState:
    """
    [contextualize ë…¸ë“œ]
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê³¼ê±° ëŒ€í™” ë§¥ë½ì´ í•„ìš”í•œì§€ íŒë‹¨.
    í•„ìš” ì‹œ chat_history_summarized ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ í›„ ì§ˆë¬¸ì„ ì¬ì‘ì„±.

    íŒë‹¨ ê¸°ì¤€ (OR ì¡°ê±´):
      1) í‚¤ì›Œë“œ ë§¤ì¹­ (ê·¸ë•Œ, ì €ë²ˆì—, ì•„ê¹Œ, â€¦)
      2) ëª¨í˜¸í•œ í‘œí˜„ ê°ì§€ (_looks_ambiguous)
      3) LLM íŒë‹¨ (recall_judgment_prompt)
    """
    messages = state.get("messages", [])
    question = _extract_question(state.get("question", "")).strip()
    # ìµœê·¼ ëŒ€í™” 10 ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    recent_chat = "\n".join(_to_text(m) for m in messages[-10:])

    # â”€â”€ 1) recall í•„ìš” ì—¬ë¶€ íŒë‹¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    keyword_recall = any(
        kw in question
        for kw in [
            "ê·¸ë•Œ", "ì €ë²ˆì—", "ì•„ê¹Œ", "ì´ì „", "ê¸°ì–µë‚˜", "ê·¸ê²Œ", "ì´ê²Œ",
            "ìœ„ì—", "ê·¸ê±°", "ë‚´ ìƒì¼", "ë‚´ ì •ë³´", "ì´ê±´", "ê·¸ê±´",
        ]
    )
    ambiguous_recall = _looks_ambiguous(question)

    llm_recall = False
    judge_prompt = f"""ë‹¹ì‹ ì€ ì§ˆì˜ ë¼ìš°íŒ… íŒë³„ê¸°ì…ë‹ˆë‹¤.
ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸ì´ ê³¼ê±° ëŒ€í™” ë§¥ë½(íŠ¹íˆ ê°œì¸ ì •ë³´/ì´ì „ ëŒ€í™” ìš”ì•½) ì—†ì´ëŠ” í•´ì„ì´ ì–´ë ¤ìš´ì§€ íŒë‹¨í•˜ì„¸ìš”.

[Recent Chat]
{recent_chat}

[Question]
{question}

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ:
YES
NO""".strip()

    try:
        text = _invoke_clean(judge_prompt).upper()
        llm_recall = "YES" in text
    except Exception:
        pass

    is_recall_needed = keyword_recall or ambiguous_recall or llm_recall
    rewrite_question = question
    long_term_context = ""

    # â”€â”€ 2) recall í•„ìš” ì‹œ â†’ ìš”ì•½ DB ê²€ìƒ‰ â†’ ì§ˆë¬¸ ì¬ì‘ì„± â”€â”€â”€â”€â”€â”€
    if is_recall_needed:
        _log("ğŸ” ê³¼ê±° ëŒ€í™” ìš”ì•½ DB ê²€ìƒ‰ ì¤‘...")

        summary_store = Chroma(
            persist_directory=PERSIST_DIR,
            collection_name=COLLECTION_CHAT_SUMMARY,
            embedding_function=_embeddings,
        )

        # ê²€ìƒ‰ ì¹œí™”ì  ì¿¼ë¦¬ ìƒì„±
        rq_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ë²¡í„° ê²€ìƒ‰í•  ì¿¼ë¦¬ë¥¼ 1 ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
- ê³¼ê±° ëŒ€í™”ì—ì„œ ì°¾ì•„ì•¼ í•  í•µì‹¬ ì—”í‹°í‹°ë¥¼ í¬í•¨í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì—†ì´ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³  ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[Recent Chat]
{recent_chat}

[Question]
{question}""".strip()

        retrieval_query = question
        try:
            cand = _invoke_clean(rq_prompt)
            if cand:
                retrieval_query = cand
        except Exception:
            pass

        docs = summary_store.similarity_search(retrieval_query, k=3)
        _log(f"retrieval_query: {retrieval_query}")
        if docs:
            long_term_context = "\n".join(d.page_content for d in docs)

        # ì§ˆë¬¸ ì¬ì‘ì„±
        rewrite_prompt = f"""You are a query rewriter.
Rewrite the user's question to be clear and standalone.
Use retrieved long-term context if available. If not available, use only recent chat.
Do not answer. Return only one rewritten question in Korean.

[Recent Chat]
{recent_chat}

[Retrieved Long-term Context]
{long_term_context}

[Original Question]
{question}""".strip()

        try:
            cand = _invoke_clean(rewrite_prompt)
            if cand:
                rewrite_question = cand
        except Exception:
            rewrite_question = question

        _log(f"ì¬ì‘ì„±ëœ ì¿¼ë¦¬: {rewrite_question}")

    return GraphState(question=rewrite_question)


def retrieve(state: GraphState) -> GraphState:
    """
    [retrieve ë…¸ë“œ]
    ChromaDB retriever ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰.
    """
    docs = _retriever.invoke(state["question"])
    return GraphState(context=format_docs(docs))


def _format_chat_history_for_prompt(messages: list, max_turns: int = MAX_HISTORY_TURNS, max_chars: int = MAX_HISTORY_CHARS) -> str:
    """
    ëŒ€í™” ì´ë ¥ì„ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜. í„´ ìˆ˜Â·ë¬¸ì ìˆ˜ ì œí•œ ì ìš©.
    - max_turns: ìµœê·¼ Ní„´(2Nê°œ ë©”ì‹œì§€)ë§Œ ì‚¬ìš©
    - max_chars: ì „ì²´ íˆìŠ¤í† ë¦¬ ë¬¸ìì—´ì´ ì´ ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ì•(ì˜¤ë˜ëœ í„´)ë¶€í„° ìƒëµ
    """
    conv = _conversation_only(messages)
    if not conv:
        return ""
    # ìµœê·¼ 2*max_turnsê°œ ë©”ì‹œì§€ = max_turns í„´
    recent = conv[-(2 * max_turns) :] if len(conv) >= 2 * max_turns else conv
    # user/assistant ìŒìœ¼ë¡œ í¬ë§· (í™€ìˆ˜ ê°œë©´ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸)
    pair_count = len(recent) // 2
    if pair_count == 0:
        return ""
    lines = []
    for i in range(pair_count):
        user_content = (recent[i * 2][1] or "").strip()
        asst_content = (recent[i * 2 + 1][1] or "").strip()
        lines.append(f"User: {user_content}\nAssistant: {asst_content}")
    history_str = "\n\n".join(lines)
    if len(history_str) <= max_chars:
        return history_str
    # ë¬¸ì ìˆ˜ ì´ˆê³¼ ì‹œ ì•ìª½ í„´ë¶€í„° ì œê±° (í•œ í„´ ë‹¨ìœ„ë¡œ)
    while len(history_str) > max_chars and len(lines) > 1:
        lines.pop(0)
        history_str = "\n\n".join(lines)
    return history_str


def llm_answer(state: GraphState) -> GraphState:
    question = state["question"]
    context = state.get("context", "")
    chat_history = state.get("messages", [])
    policy = state.get("policy", "")

    style = route_style(question)  # centroids â†’ direct/socratic/scaffolding/feedback
    _log(f"ğŸ¯ LoRA ë¼ìš°íŒ…: {style}")
    # PEFT ë©€í‹° ì–´ëŒ‘í„°ì¼ ë•Œë§Œ ì¿¼ë¦¬ë³„ ì–´ëŒ‘í„° ìŠ¤ìœ„ì¹­ (ì „ì²´ ëª¨ë¸ ë‹¨ì¼ ë¡œë“œ ì‹œ ë¬´ì‹œ)
    if _peft_model is not None:
        try:
            _peft_model.set_adapter(style)
        except (ValueError, KeyError) as e:
            _log(f"âš ï¸ ì–´ëŒ‘í„° '{style}' ì ìš© ì‹¤íŒ¨ ({e}) â†’ direct ì‚¬ìš©")
            style = "direct"
            _peft_model.set_adapter(style)

    history_str = _format_chat_history_for_prompt(chat_history)
    if history_str:
        history_block = f"History:\n{history_str}\n\n"
    else:
        history_block = ""

    prompt = f"""<|im_start|>system
You are a helpful tutor. Use the context below only to inform your answer. Do not repeat or output the words "system", "Context:", "Policy:", "History:", "user", "assistant" or any URLs. Reply only with the assistant's answer in natural language.
{history_block}Context: {context}
Policy: {policy}
<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
    try:
        response = _rag_llm.invoke(prompt).strip()
        response = _clean_answer_for_display(response)
    except Exception as e:
        _log(f"âŒ ìƒì„± ì‹¤íŒ¨: {e}")
        response = "ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ."

    return GraphState(
        answer=response,
        messages=[("user", question), ("assistant", response)],
    )



def relevance_check(state: GraphState) -> GraphState:
    """
    [relevance_check ë…¸ë“œ]
    ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ _chat_hf ë¡œ í‰ê°€.
    ê²°ê³¼ë¥¼ state['relevance'] = 'yes' | 'no' ë¡œ ì €ì¥.
    """
    prompt = f"""You are a grader assessing whether a retrieved document is relevant to the given question.
Return ONLY valid JSON like: {{"score": "yes"}} or {{"score": "no"}}.

Question:
{state["question"]}

Retrieved document:
{state["context"]}""".strip()

    text = _invoke_clean(prompt)

    # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ëª¨ë¸ì´ ì•ë’¤ì— í…ìŠ¤íŠ¸ë¥¼ ì„ëŠ” ê²½ìš° ëŒ€ë¹„)
    match = re.search(r"\{.*\}", text, re.DOTALL)
    if match:
        text = match.group(0)

    try:
        data = json.loads(text)
        score = data.get("score", "no").lower()
    except Exception:
        score = "no"

    if score not in ("yes", "no"):
        score = "no"

    _log(f"ğŸ“‹ ê´€ë ¨ì„± í‰ê°€: {score}")
    return {"relevance": score}


def web_search(state: GraphState) -> GraphState:
    """
    [web_search ë…¸ë“œ]
    Tavily API ë¡œ ì›¹ ê²€ìƒ‰ í›„ ê²°ê³¼ë¥¼ context ì— ì €ì¥.
    ê²€ìƒ‰ ê²°ê³¼ëŠ” ChromaDB(my_collection)ì—ë„ ì ì¬í•˜ì—¬ ì¬í™œìš©.
    """
    _log("ğŸŒ ì›¹ ê²€ìƒ‰ ì‹œì‘...")
    tavily = TavilySearch(max_results=5, search_depth="basic")
    query_text = state["question"]
    try:
        results = tavily.invoke(query_text)
    except Exception as e:
        _log(f"âš ï¸ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return GraphState(context="ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # TavilySearchëŠ” ë²„ì „ì— ë”°ë¼ list / dict / str ì„ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
    if isinstance(results, dict):
        results = results.get("results", [results])
    if isinstance(results, str):
        results = [{"content": results}]
    if not isinstance(results, list):
        _log(f"âš ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ë¶ˆê°€: {type(results)}")
        return GraphState(context="ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    parts = []
    for r in results:
        if isinstance(r, dict):
            url = r.get("url", "")
            content = _summarize_if_long(r.get("content", ""))
            parts.append(f"{content}\nì¶œì²˜: {url}" if url else content)
        else:
            parts.append(_summarize_if_long(str(r)))
    formatted = "\n\n---\n\n".join(parts)

    # ChromaDB ì—ë„ ì €ì¥ (ë¹„ë™ê¸° â€” ë…¸ë“œ ë°˜í™˜ì„ ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŒ)
    if formatted.strip():
        doc = Document(
            page_content=formatted,
            metadata={
                "source": f"web_search:{query_text}",
                "origin": "tavily_merged",
            },
        )

        def _bg_save_web():
            try:
                _raw_ingest_docs(
                    documents=[doc],
                    persist_directory=PERSIST_DIR,
                    collection_name=COLLECTION_MAIN,
                )
                _log("âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ ChromaDB ì €ì¥ ì™„ë£Œ (bg)")
            except Exception as e:
                _log(f"âš ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ (bg): {e}")

        _bg_executor.submit(_bg_save_web)

    return GraphState(context=formatted)


def save_memory(state: GraphState) -> GraphState:
    """
    [save_memory ë…¸ë“œ]
    10í„´ë§ˆë‹¤ ì‹¤í–‰ë˜ì–´:
      1) ì˜¤ë˜ëœ ëŒ€í™” 10í„´(20ê°œ ë©”ì‹œì§€)ì„ raw ì»¬ë ‰ì…˜ì— ì €ì¥
      2) ìµœê·¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ 10í„´ê°„ì˜ ë‹µë³€ ë°©í–¥ì„±(policy)ì„ ìƒì„±
      3) ì €ì¥í•œ 10í„´ì€ state.messagesì—ì„œ ì œê±°í•˜ì—¬ ì´í›„ context/promptì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ í•¨
    """
    messages = state.get("messages", [])
    conv = _conversation_only(messages)
    MIN_MSGS = 20  # 10 í„´ = 20 ë©”ì‹œì§€

    if len(conv) < MIN_MSGS:
        _log(f"â„¹ï¸ save_memory ê±´ë„ˆëœ€: ëŒ€í™” {len(conv)}ê°œ (< {MIN_MSGS})")
        return {}

    oldest = conv[:MIN_MSGS]
    raw_text = "\n".join(f"{r}: {c}" for r, c in oldest).strip()
    if not raw_text:
        return {}

    ts = datetime.now(timezone.utc).isoformat()
    mem_id = uuid.uuid4().hex

    # â”€â”€ raw ëŒ€í™” ì €ì¥ (10í„´) â”€â”€
    raw_doc = Document(
        page_content=raw_text,
        metadata={
            "source": "chat_history_raw",
            "memory_id": mem_id,
            "saved_at": ts,
            "turn_count": 10,
            "message_count": MIN_MSGS,
        },
    )

    delete_messages = [RemoveMessage(id=msg.id) for msg in messages[:MIN_MSGS]]
    _log(f"ğŸ§¹ ë©”ì‹œì§€ ì •ë¦¬: {MIN_MSGS}ê°œ ì œê±°")
    
    return {
        "policy": policy_text,
        "messages": delete_messages  # add_messages ë¦¬ë“€ì„œê°€ ìë™ ì²˜ë¦¬
    }

    def _bg_save_raw():
        try:
            _raw_ingest_docs(
                documents=[raw_doc],
                persist_directory=PERSIST_DIR,
                collection_name=COLLECTION_CHAT_RAW,
                chunk_size=1200,
                chunk_overlap=120,
            )
            _log("âœ… save_memory raw ì €ì¥ ì™„ë£Œ (bg)")
        except Exception as e:
            _log(f"âš ï¸ save_memory raw ì €ì¥ ì‹¤íŒ¨ (bg): {e}")

    _bg_executor.submit(_bg_save_raw)

    # â”€â”€ policy ìƒì„± (ìµœê·¼ ëŒ€í™” ê¸°ë°˜) â”€â”€
    recent = conv[-20:]  # ìµœê·¼ 10í„´
    conv_text = "\n".join(f"{r}: {c}" for r, c in recent).strip()

    policy_prompt = f"""ë‹¹ì‹ ì€ í•™ìŠµ íŠœí„°ì˜ êµìœ¡ ì „ëµ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ í•™ìƒê³¼ íŠœí„°ì˜ ìµœê·¼ ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬, ì•ìœ¼ë¡œ 10í„´ ë™ì•ˆ íŠœí„°ê°€ ì·¨í•´ì•¼ í•  ë‹µë³€ ë°©í–¥ì„±(policy)ì„ ê²°ì •í•˜ì„¸ìš”.

[ìµœê·¼ ëŒ€í™” ë‚´ìš©]
{conv_text}

ì•„ë˜ ë³´ê¸° ì¤‘ì—ì„œ í•™ìƒì—ê²Œ ê°€ì¥ ì í•©í•œ ë°©í–¥ì„±ì„ 1~2ê°œ ì„ íƒí•˜ê³ , í•´ë‹¹ í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
ì—¬ëŸ¬ ê°œ ì„ íƒ ì‹œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.

- ê°œë… ì´í•´ ë¶€ì¡± -> ì˜ˆì‹œë¥¼ í†µí•œ ê°œë… ì„¤ëª…
- ì‘ìš©ëŠ¥ë ¥ ë¶€ì¡± -> ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ
- ì•”ê¸° ëŠ¥ë ¥ ê°•í™” -> ì•ê¸€ìë¥¼ ë”° ì•”ê¸°ë°©ì‹ ì¶”ì²œ
- ê°œë… ê°„ ì—°ê²° ë¶€ì¡± -> ì—°ê´€ ê°œë… ë° ë¹„êµ ì„¤ëª…
- ìì£¼ í‹€ë¦¬ëŠ” ìœ í˜• -> ì˜¤ë‹µ ë¶„ì„ ë° ë°˜ë³µ í•™ìŠµ ìœ ë„
- ì‹¬í™” í•™ìŠµ í•„ìš” -> ë‚œì´ë„ ë†’ì€ ì§ˆë¬¸ ìœ ë„
- ê¸°ì´ˆ ë¶€ì¡± -> ì„ ìˆ˜ ê°œë…ë¶€í„° ë‹¨ê³„ì  ì„¤ëª…

ë°©í–¥ì„±ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.""".strip()

    try:
        policy_text = _invoke_clean(policy_prompt)
    except Exception as e:
        _log(f"âš ï¸ policy ìƒì„± ì‹¤íŒ¨: {e}")
        policy_text = state.get("policy", "")

    _log(f"ğŸ“‹ policy ê°±ì‹ : {policy_text}")

    # â”€â”€ ì €ì¥í•œ 10í„´(20ê°œ ë©”ì‹œì§€)ì„ state.messagesì—ì„œ ì œê±° â”€â”€
    # add_messages ë¦¬ë“€ì„œ: REMOVE_ALL í›„ ë‚¨ê¸¸ ë©”ì‹œì§€ë§Œ ë‹¤ì‹œ ë„£ìœ¼ë©´, ì´í›„ context/promptì— ì €ì¥ë¶„ì´ í¬í•¨ë˜ì§€ ì•ŠìŒ
    remaining = list(messages[MIN_MSGS:])
    new_messages = [RemoveMessage(id=REMOVE_ALL_MESSAGES)] + remaining
    _log(f"ğŸ§¹ ë©”ì‹œì§€ ì •ë¦¬: ì €ì¥í•œ {MIN_MSGS}ê°œ ì œê±°, {len(remaining)}ê°œë§Œ ìœ ì§€")

    return {"policy": policy_text, "messages": new_messages}


# ============================================================
# 12. ë¼ìš°íŒ… í•¨ìˆ˜ (conditional_edges ìš©)
# ============================================================


def retrieve_or_not(state: GraphState) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ê²€ìƒ‰(retrieve)ì´ í•„ìš”í•œì§€ LLM ìœ¼ë¡œ íŒë‹¨.
    - ê²€ìƒ‰ ë¶ˆí•„ìš” â†’ "not retrieve" â†’ llm_answer ì§í–‰
    - ê²€ìƒ‰ í•„ìš”   â†’ "retrieve"     â†’ retrieve ë…¸ë“œ
    """
    question = state.get("question", "")
    if not question:
        return "not retrieve"

    prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ë ¤ë©´ **ë¬¸ì„œ/ë²¡í„°DB ê²€ìƒ‰(retrieve)**ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

íŒë‹¨ ê¸°ì¤€:
- ì¸ì‚¬, ê°ì •, ë‹¨ìˆœ ëŒ€í™”("ì•ˆë…•", "ê³ ë§ˆì›Œ", "ë­í•´" ë“±), ì¡ë‹´ â†’ ê²€ìƒ‰ ë¶ˆí•„ìš”
- ë¬¸ì„œì— ìˆì„ ë²•í•œ ì „ë¬¸ ì§€ì‹ ì§ˆë¬¸ â†’ ê²€ìƒ‰ í•„ìš”
- ìµœì‹  ì •ë³´/ë‰´ìŠ¤ â†’ ê²€ìƒ‰ í•„ìš”

ì§ˆë¬¸: {question}

*ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSON ë§Œ ì¶œë ¥.
{{"need_retrieve": "yes"}} ë˜ëŠ” {{"need_retrieve": "no"}}""".strip()

    try:
        text = _invoke_clean(prompt)
        match = re.search(r'\{[^{}]*"need_retrieve"[^{}]*\}', text)
        if match:
            data = json.loads(match.group(0))
            need = (data.get("need_retrieve") or "no").lower()
            if need in ("yes", "true", "1"):
                _log("ğŸ“– â†’ retrieve ë…¸ë“œë¡œ ì´ë™")
                return "retrieve"
        _log("ğŸ’¬ â†’ llm_answer ë…¸ë“œë¡œ ì§í–‰")
        return "not retrieve"
    except Exception:
        return "retrieve"  # ì—ëŸ¬ ì‹œ ì•ˆì „í•˜ê²Œ ê²€ìƒ‰ ì‹¤í–‰


def is_relevant(state: GraphState) -> str:
    """ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°"""
    return "relevant" if state.get("relevance") == "yes" else "not relevant"


def save_or_not(state: GraphState) -> str:
    """ëŒ€í™”ê°€ 10í„´ ë‹¨ìœ„(20 ë©”ì‹œì§€)ì¼ ë•Œ save_memory ë¡œ ë¶„ê¸°í•˜ì—¬ policy ê°±ì‹ """
    conv = _conversation_only(state.get("messages", []))
    turn_count = len(conv) // 2
    if turn_count > 0 and turn_count % 10 == 0:
        return "save_chat"
    return "too short"


# ============================================================
# 13. ê·¸ë˜í”„ êµ¬ì„± Â· ì»´íŒŒì¼
# ============================================================


def build_app():
    """
    LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ê³  ì»´íŒŒì¼í•œë‹¤.

    ê·¸ë˜í”„ êµ¬ì¡°:
      START â†’ contextualize
              â”œâ”€ (retrieve í•„ìš”)  â†’ retrieve â†’ relevance_check
              â”‚                                â”œâ”€ (relevant)     â†’ llm_answer
              â”‚                                â””â”€ (not relevant) â†’ web_search â†’ llm_answer
              â””â”€ (retrieve ë¶ˆí•„ìš”) â†’ llm_answer
                                     â”œâ”€ (save_chat) â†’ save_memory â†’ END
                                     â””â”€ (too short) â†’ END

    Returns:
        ì»´íŒŒì¼ëœ LangGraph ì•±
    """
    global _app

    workflow = StateGraph(GraphState)

    # # â”€â”€ ë…¸ë“œ ë“±ë¡ â”€â”€
    workflow.add_node("contextualize", _timed_node(contextualize, "contextualize"))
    workflow.add_node("save_memory", _timed_node(save_memory, "save_memory"))
    workflow.add_node("retrieve", _timed_node(retrieve, "retrieve"))
    workflow.add_node("llm_answer", _timed_node(llm_answer, "llm_answer"))
    workflow.add_node("relevance_check", _timed_node(relevance_check, "relevance_check"))
    workflow.add_node("web_search", _timed_node(web_search, "web_search"))

    # â”€â”€ ì§„ì…ì  â”€â”€
    workflow.set_entry_point("contextualize")

    # â”€â”€ ì¡°ê±´ë¶€ ì—£ì§€: contextualize â†’ retrieve | llm_answer â”€â”€
    workflow.add_conditional_edges(
        "contextualize",
        retrieve_or_not,
        {"retrieve": "retrieve", "not retrieve": "llm_answer"},
    )

    # â”€â”€ retrieve â†’ relevance_check â”€â”€
    workflow.add_edge("retrieve", "relevance_check")

    # â”€â”€ ì¡°ê±´ë¶€ ì—£ì§€: relevance_check â†’ llm_answer | web_search â”€â”€
    workflow.add_conditional_edges(
        "relevance_check",
        is_relevant,
        {"relevant": "llm_answer", "not relevant": "web_search"},
    )

    # â”€â”€ web_search â†’ llm_answer â”€â”€
    workflow.add_edge("web_search", "llm_answer")

    # â”€â”€ ì¡°ê±´ë¶€ ì—£ì§€: llm_answer â†’ save_memory | END â”€â”€
    workflow.add_conditional_edges(
        "llm_answer",
        save_or_not,
        {"save_chat": "save_memory", "too short": END},
    )

    # â”€â”€ save_memory â†’ END â”€â”€
    workflow.add_edge("save_memory", END)

    # â”€â”€ ì»´íŒŒì¼ (MemorySaver: ì¸ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„°) â”€â”€
    memory = MemorySaver()
    _app = workflow.compile(checkpointer=memory)
    _log("âœ… LangGraph ì•± ì»´íŒŒì¼ ì™„ë£Œ")
    return _app


# ============================================================
# 14. ê³µê°œ API
# ============================================================


def query(question: str, thread_id: str | None = None) -> Dict[str, Any]:
    """
    ì§ˆë¬¸ì„ ì‹¤í–‰í•˜ê³  ìµœì¢… GraphState ë¥¼ ë°˜í™˜.

    Args:
        question:  ì‚¬ìš©ì ì§ˆë¬¸ ë¬¸ìì—´
        thread_id: ëŒ€í™” ì„¸ì…˜ ID (None ì´ë©´ ìë™ ìƒì„±)

    Returns:
        ìµœì¢… ìƒíƒœ ë”•ì…”ë„ˆë¦¬ (question, context, answer, messages, relevance)
    """
    if _app is None:
        raise RuntimeError("build_app() ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”")

    if thread_id is None:
        thread_id = random_uuid()

    config = RunnableConfig(
        recursion_limit=10,
        configurable={"thread_id": thread_id},
    )
    inputs = GraphState(question=question)

    # stream ëª¨ë“œë¡œ ì‹¤í–‰ â€” ê° ë…¸ë“œ ì™„ë£Œ ì‹œ ë¡œê·¸
    for event in _app.stream(inputs, config=config):
        for node_name in event:
            _log(f"ğŸ”„ {node_name} ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ")

    return _app.get_state(config).values


def get_app():
    """ì»´íŒŒì¼ëœ LangGraph ì•± ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return _app

def is_initialized() -> bool:
    """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ ì—¬ë¶€"""
    return _initialized


def verify_chain_and_lora_config() -> Dict[str, Any]:
    """
    ì²´ì¸ ìƒì„±Â·LoRA ë¼ìš°íŒ… ì„¤ì •ì´ ì •ìƒì¸ì§€ ê²€ì¦ (ëª¨ë¸ ë¡œë“œ ì—†ì´ ì„¤ì •ë§Œ ê²€ì‚¬).
    - router_model.json centroids í‚¤ì™€ STYLE_MODELS í‚¤ ì¼ì¹˜ ì—¬ë¶€
    - ì–´ëŒ‘í„° ì´ë¦„ ì¼ì¹˜ ì‹œ set_adapter(style) ì •ìƒ ë™ì‘ ê°€ëŠ¥ ì—¬ë¶€
    Returns:
        {"ok": bool, "checks": {...}, "errors": [...]}
    """
    result = {"ok": True, "checks": {}, "errors": []}

    # 1) STYLE_MODELS í‚¤ = ë¼ìš°í„°/ì–´ëŒ‘í„°ì—ì„œ ì“°ëŠ” ì´ë¦„
    style_keys = set(STYLE_MODELS.keys())
    result["checks"]["style_keys"] = list(style_keys)
    if "direct" not in style_keys:
        result["ok"] = False
        result["errors"].append("STYLE_MODELSì— 'direct'ê°€ ì—†ìŒ")

    # 2) router_model.json ì¡´ì¬ ë° centroids í‚¤ì™€ STYLE_MODELS ì¼ì¹˜
    if not LORA_ROUTER_PATH.exists():
        result["checks"]["router_file"] = "ì—†ìŒ (route_styleì€ direct ë°˜í™˜)"
    else:
        try:
            with open(LORA_ROUTER_PATH, "r") as f:
                data = json.load(f)
            centroids = data.get("centroids") or data.get("styles", [])
            if isinstance(centroids, dict):
                centroid_keys = set(centroids.keys())
            else:
                centroid_keys = set(centroids) if isinstance(centroids, list) else set()
            result["checks"]["centroid_keys"] = list(centroid_keys)
            missing_in_router = style_keys - centroid_keys
            if missing_in_router:
                result["ok"] = False
                result["errors"].append(f"router_modelì— ì—†ëŠ” ìŠ¤íƒ€ì¼: {missing_in_router}")
            extra = centroid_keys - style_keys
            if extra:
                result["checks"]["extra_in_router"] = list(extra)
        except Exception as e:
            result["ok"] = False
            result["errors"].append(f"router_model.json ë¡œë“œ ì‹¤íŒ¨: {e}")

    # 3) ì´ˆê¸°í™” ì‹œ ì²« ì–´ëŒ‘í„° ì´ë¦„ì´ "direct"ë¡œ ë¡œë“œë˜ë¯€ë¡œ route_style("direct") â†’ set_adapter("direct") ì¼ì¹˜
    result["checks"]["adapter_name_note"] = "PeftModel.from_pretrained(..., adapter_name='direct') ì‚¬ìš©ìœ¼ë¡œ direct ì¼ì¹˜"

    return result


if __name__ == "__main__":
    import sys
    r = verify_chain_and_lora_config()
    print("ì²´ì¸Â·LoRA ì„¤ì • ê²€ì¦:", "OK" if r["ok"] else "ì‹¤íŒ¨")
    for k, v in r["checks"].items():
        print(f"  {k}: {v}")
    if r["errors"]:
        for e in r["errors"]:
            print("  ì˜¤ë¥˜:", e)
        sys.exit(1)
    print("(ì‹¤ì œ ì²´ì¸ ìƒì„±Â·ì–´ëŒ‘í„° ì ìš©ì€ initialize() â†’ build_app() â†’ query() í˜¸ì¶œ ì‹œ ìˆ˜í–‰)")
